# Attention Attribution Matrix Data Schema

## Overview
This document describes the data schema for attention attribution matrices produced by the GPT-2 language model. These matrices capture how each output token attends to each input token during the text generation process, providing insights into the model's internal attention patterns.

## Data Structure
The data is provided in JSON format with the following top-level fields:

| Field | Type | Description |
|-------|------|-------------|
| `model` | String | The language model used (e.g., "GPT-2") |
| `method` | String | The attribution method used (e.g., "attention") |
| `file_id` | Integer | A unique identifier for the data file |
| `prompt` | String | The original input text provided to the model |
| `generation` | String | The text generated by the model in response to the prompt |
| `source_tokens` | Array of Strings | The tokenized representation of the input prompt |
| `target_tokens` | Array of Strings | The tokenized representation of the model's output |
| `attribution_matrix` | 2D Array of Floats | The matrix of attention values between output and input tokens |
| `aggregation` | String | The method used to aggregate attention scores (e.g., "sum") |

## Detailed Field Specifications

### `model`
String identifier for the language model variant.
- Example: "GPT-2"
- Possible values may include model size variants like "GPT-2-small", "GPT-2-medium", etc.

### `method`
String identifier for the attribution method used to generate the matrix.
- Example: "attention"
- Other possible values might include "gradient", "integrated-gradients", etc.

### `file_id`
Numeric identifier for the data file, typically an integer starting from 0.
- Example: 0

### `prompt`
The raw text string provided as input to the language model.
- Example: "Answer this yes/no question: do iran and afghanistan speak the same language"

### `generation`
The raw text string generated by the model in response to the prompt.
- Example: "Answer this yes/no question: do iran and afghanistan speak the same language?\n\nAnswer this yes/no question: do iran and afghanistan speak the same"

### `source_tokens`
An array of strings representing the tokenized version of the input prompt. Each element is a token as processed by the model's tokenizer.
- Example: `["Answer", "Ġthis", "Ġyes", "/", "no", ...]`
- Note: Tokens prefixed with "Ġ" represent tokens that begin with a space in the original text.
- Length: Variable, depends on the input prompt length
- In the example data: 20 tokens

### `target_tokens`
An array of strings representing the tokenized version of the generated output. Each element is a token as processed by the model's tokenizer.
- Example: `["Answer", "Ġthis", "Ġyes", "/", "no", ...]`
- Note: Tokens prefixed with "Ġ" represent tokens that begin with a space in the original text.
- Special tokens like "Ċ" may represent newlines or other special characters.
- Length: Variable, depends on the generation length
- In the example data: 38 tokens

### `attribution_matrix`
A 2D array (matrix) of floating-point values that represent the attention scores.
- Dimensions: `[target_tokens.length] × [source_tokens.length]`
- In the example: 38 rows × 20 columns
- Row index corresponds to position in `target_tokens`
- Column index corresponds to position in `source_tokens`
- Values: Positive floating-point numbers (typically ranging from 0 to ~80 in the example)
- Higher values indicate stronger attention between the corresponding output and input tokens
- The matrix has a triangular structure in its lower portion:
  - Upper section: Fully populated with values
  - Lower section: Contains zeros in a triangular pattern
  - This triangular structure reflects the causal nature of the attention mechanism (tokens can only attend to previous tokens, not future ones)

#### Special Properties of the Attribution Matrix
1. **Causality Constraint**: The matrix has a triangular pattern of zeros in its lower portion, representing the causal constraint of the attention mechanism. When generating token i, the model can only attend to tokens that came before it.

2. **Sliding Window**: The triangular boundary indicates that each output token attends only to a fixed number of previous tokens, creating a sliding attention window.

3. **Value Range**: Attribution values are non-negative floating-point numbers, typically ranging from 0 (no attention) to higher values (strong attention). In the example data, values range from approximately 0 to 78.2.

4. **Diagonal Patterns**: Often displays strong diagonal-like patterns where tokens attend heavily to themselves or positionally similar tokens.

### `aggregation`
String identifier for the method used to aggregate attention scores across multiple attention heads or layers.
- Example: "sum"
- Other possible values might include "mean", "max", etc.

## Visualization Considerations

When implementing visualizations for this data, consider the following approaches:

1. **Heatmap Visualization**:
   - Map attention scores to color intensity
   - Use a colormap that clearly shows the range of values (e.g., viridis, plasma)
   - Ensure the zero values in the triangular pattern are visually distinct

2. **Token Alignment**:
   - Display source tokens along the x-axis and target tokens along the y-axis
   - Consider rotating token labels for readability
   - Provide hover tooltips that show exact attention values and the corresponding tokens

3. **Highlighting Patterns**:
   - Implement interactive features to highlight diagonal patterns
   - Allow toggling between raw and normalized attention values
   - Provide options to filter by attention threshold

4. **Time-series View**:
   - Create an animated view that shows how attention shifts as each token is generated
   - Highlight the "sliding window" effect as generation progresses

5. **Attention Flow Visualization**:
   - Visualize the flow of attention from output tokens to input tokens using arrows or curves
   - Thickness of connections can represent attention strength

6. **Triangular Structure Handling**:
   - Decide whether to display the zero values in the triangular pattern or crop them out
   - If displaying, use a distinct color (e.g., black or gray) to differentiate zeros from low attention values

## Example Implementation Pseudocode

```javascript
// Basic heatmap visualization pseudocode
function visualizeAttentionMatrix(data) {
  const sourceTokens = data.source_tokens;
  const targetTokens = data.target_tokens;
  const matrix = data.attribution_matrix;
  
  // Find min/max for color scaling (excluding zeros)
  let min = Infinity;
  let max = -Infinity;
  for (let i = 0; i < matrix.length; i++) {
    for (let j = 0; j < matrix[i].length; j++) {
      if (matrix[i][j] > 0) {
        min = Math.min(min, matrix[i][j]);
        max = Math.max(max, matrix[i][j]);
      }
    }
  }
  
  // Create color scale
  const colorScale = d3.scaleSequential()
    .domain([0, max])
    .interpolator(d3.interpolateViridis);
    
  // Draw heatmap cells
  for (let i = 0; i < matrix.length; i++) {
    for (let j = 0; j < matrix[i].length; j++) {
      const value = matrix[i][j];
      const color = value === 0 ? '#222' : colorScale(value);
      
      drawRect(j, i, 1, 1, {
        fill: color,
        stroke: 'none',
        opacity: 0.9
      });
      
      // Add interactive tooltip
      addTooltip(`Output token: ${targetTokens[i]}\nInput token: ${sourceTokens[j]}\nAttention: ${value.toFixed(2)}`);
    }
  }
  
  // Add axis labels
  drawXAxis(sourceTokens);
  drawYAxis(targetTokens);
  
  // Add title and legend
  drawTitle(`Attention Attribution for "${data.model}" using "${data.method}"`);
  drawColorLegend(0, max, colorScale);
}
```

## Data Loading Considerations

When implementing a visualization tool for this data:

1. **File Size**: Attribution matrices can be large for longer texts. Implement progressive loading or downsampling for very large matrices.

2. **Data Validation**: Verify that the dimensions of the attribution matrix match the lengths of source_tokens and target_tokens arrays.

3. **Preprocessing**: Consider normalizing the attribution values (e.g., row-wise or column-wise) to highlight relative attention patterns.

4. **Caching**: Implement caching mechanisms for processed data to improve performance when interactively exploring the visualization.

5. **Sparse Representation**: For very large matrices, consider using a sparse representation to save memory, especially since many values may be zero in the triangular pattern.